[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "slides_2.html#agenda",
    "href": "slides_2.html#agenda",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Agenda",
    "text": "Agenda\n\n\nMotivation: Why we need supervised dimensionality reduction\nPCA: Find directions of variance in X\nPLS-DA: Use PLS for classification tasks\nPLS: Maximize covariance between X and Y\nValidation: R², Q², cross-validation\nInterpretation: Loadings, scores, VIP scores\nData Integration: Multi-omics with block-wise PLS"
  },
  {
    "objectID": "slides_2.html#why-learn-pls",
    "href": "slides_2.html#why-learn-pls",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Why Learn PLS?",
    "text": "Why Learn PLS?\n\n\nHigh-dimensional data: \\(p \\gg n\\)\nStrong correlations among features\nNeed interpretable predictive components\nWant to relate X (predictors) to Y (response) where both can be matrices\nOmics data (genomics, transcriptomics, proteomics)\nChemometrics and quality control\nData integration across multiple blocks (multi-omics, multi-view)"
  },
  {
    "objectID": "slides_2.html#pca",
    "href": "slides_2.html#pca",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PCA",
    "text": "PCA\n\n\n\nPCA rotates the data so that the new axes align with the directions of maximum variance.\nThis does not mean that this is going to be aligned with your expectation!\n\n\n\nlibrary(mixOmics)\ndata(srbct)\nX &lt;- srbct$gene \nY &lt;- srbct$class\npca_model&lt;-pca(X,ncomp = 2,center = T,scale = T)\nplotIndiv(pca_model,group=Y,ind.names = FALSE,ellipse = FALSE,legend = TRUE,title = \"PCA\")"
  },
  {
    "objectID": "slides_2.html#pca-1",
    "href": "slides_2.html#pca-1",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PCA",
    "text": "PCA\n\n\n\nPCA finds directions of maximum variance, but those directions might not separate the classes.\nYou might see clusters in the PCA plot… or you might not.\nWhat if the relevant variation is small compared to the noise or other irrelevant signals?\nWhat if we know the class labels and want to use them to guide our analysis?\nPCA ignores Y completely — it’s unsupervised.\n\nSo… what if we want to rotate the data toward separation, not just variance?\n\n\nlibrary(mixOmics)\ndata(srbct)\nX &lt;- srbct$gene \nY &lt;- srbct$class\npca_model&lt;-pca(X,ncomp = 2,center = T,scale = T)\nplotIndiv(pca_model,group=Y,ind.names = FALSE,ellipse = FALSE,legend = TRUE,title = \"PCA\")"
  },
  {
    "objectID": "slides_2.html#pls-da",
    "href": "slides_2.html#pls-da",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PLS-DA",
    "text": "PLS-DA\n\n\n\nPLS-DA rotates the data to align with the directions that best separate the groups (maximize covariance between predictors and class labels).\nThis means that unlike PCA, PLS-DA uses the class labels to guide the new axes.\n\n\n\nlibrary(mixOmics)\ndata(srbct)\nX &lt;- srbct$gene \nY &lt;- srbct$class\nhead(Y)\n\n[1] EWS EWS EWS EWS EWS EWS\nLevels: EWS BL NB RMS\n\nplsda_model&lt;-plsda(X,Y,ncomp = 2,scale = T)\nplotIndiv(plsda_model,group=Y,ind.names = FALSE,ellipse = FALSE,legend = TRUE,title = \"PLS-DA\")"
  },
  {
    "objectID": "slides_2.html#pls-da-1",
    "href": "slides_2.html#pls-da-1",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PLS-DA",
    "text": "PLS-DA\n\n\nPLS-DA is a supervised method. This means it can be used for:\n\nPrediction, not only exploration.\nClassifying new samples based on their position in the component space.\nUnderstanding which variables contribute most to class separation.\nBuilding discriminant models guided by known group labels.\nEvaluating performance with cross-validation, accuracy, ROC, or confusion matrices.\nIdentifying biomarkers or features associated with group differences.\n\n\n\nlibrary(mixOmics)\ndata(srbct)\nX &lt;- srbct$gene \nY &lt;- srbct$class\nhead(Y)\n\n[1] EWS EWS EWS EWS EWS EWS\nLevels: EWS BL NB RMS\n\nplsda_model&lt;-plsda(X,Y,ncomp = 2,scale = T)\nplotIndiv(plsda_model,group=Y,ind.names = FALSE,ellipse = FALSE,legend = TRUE,title = \"PLS-DA\")"
  },
  {
    "objectID": "slides_2.html#pls-da-2",
    "href": "slides_2.html#pls-da-2",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PLS-DA",
    "text": "PLS-DA\n\n\nTo do PLS-DA, you need:\n\nA matrix X of predictors (e.g. gene expression, metabolite levels).\nA vector Y of class labels (e.g. disease status, cell types).\nTo encode Y as a factor for classification.\nTo decide on the number of components ncomp to extract.\nPreprocessing likescaling.\n\n\n\nlibrary(mixOmics)\ndata(srbct)\nX &lt;- srbct$gene \nY &lt;- srbct$class\nplsda_model&lt;-plsda(X,Y,ncomp = 2,scale = T)\nplotIndiv(plsda_model,group=Y,ind.names = FALSE,ellipse = FALSE,legend = TRUE,title = \"PLS-DA\")"
  },
  {
    "objectID": "slides_2.html#pls-da-3",
    "href": "slides_2.html#pls-da-3",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PLS-DA",
    "text": "PLS-DA\n\n\nPLS-DA will:\n\nOutput scores (for plotting): These components are chosen to maximize the covariance between the predictors X and the class labels Y.\n\n\n\nlibrary(mixOmics)\ndata(srbct)\nX &lt;- srbct$gene \nY &lt;- srbct$class\nplsda_model&lt;-plsda(X,Y,ncomp = 2,scale = T)\nplotIndiv(plsda_model,group=Y,ind.names = FALSE,ellipse = FALSE,legend = TRUE,title = \"PLS-DA\")"
  },
  {
    "objectID": "slides_2.html#pls-da-4",
    "href": "slides_2.html#pls-da-4",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PLS-DA",
    "text": "PLS-DA\n\n\nPLS-DA will:\n\nOutput scores (for plotting)\nOutput loadings: indicate how much each original variable contributes to the components that maximize covariance between X and Y.\n\n\n\nlibrary(mixOmics)\ndata(srbct)\nX &lt;- srbct$gene \nY &lt;- srbct$class\nplsda_model&lt;-plsda(X,Y,ncomp = 2,scale = T)\nplotLoadings(plsda_model,comp = 1)"
  },
  {
    "objectID": "slides_2.html#pls-da-5",
    "href": "slides_2.html#pls-da-5",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PLS-DA",
    "text": "PLS-DA\n\n\nPLS-DA will:\n\nOutput scores (for plotting)\nOutput loadings\nFind important variables: VIP scores summarize the overall importance of each variable in explaining the variation in Y across all PLS-DA components. Often VIP &gt; 1 is considered a good hit!\n\n\n\nlibrary(mixOmics)\ndata(srbct)\nX &lt;- srbct$gene \nY &lt;- srbct$class\nplsda_model&lt;-plsda(X,Y,ncomp = 2,scale = T)\nhead(vip(plsda_model))\n\n       comp1     comp2\ng1 2.4244166 1.6125884\ng2 0.8565739 2.0220269\ng3 1.1781767 1.1703119\ng4 1.3704382 0.9357488\ng5 0.7790556 0.5680709\ng6 0.9253340 0.6224794"
  },
  {
    "objectID": "slides_2.html#pls-da-6",
    "href": "slides_2.html#pls-da-6",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PLS-DA",
    "text": "PLS-DA\n\n\nPLS-DA will:\n\nOutput scores (for plotting)\nOutput loadings\nFind important variables\nLet you predict new samples and show you the decision boundries\n\n\n\nlibrary(mixOmics)\ndata(srbct)\nX &lt;- srbct$gene[-c(1:5),] \nY &lt;- srbct$class[-c(1:5)]\nplsda_model&lt;-plsda(X,Y,ncomp = 2,scale = T)\npredictions&lt;-predict(plsda_model,X[1:5,])\ntable(predictions$class$mahalanobis.dist[,2],Y[1:5])\n\n     \n      EWS BL NB RMS\n  EWS   5  0  0   0\n\nbackground = background.predict(plsda_model, comp.predicted=2, dist = \"mahalanobis.dist\")\nplotIndiv(plsda_model, comp = 1:2,\n          group =Y, ind.names = FALSE,\n          background = background, \n          legend = TRUE, title = \"PLSDA with prediction background\")"
  },
  {
    "objectID": "slides_2.html#pls-da-needs-to-be-validated",
    "href": "slides_2.html#pls-da-needs-to-be-validated",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PLS-DA needs to be validated",
    "text": "PLS-DA needs to be validated\n\n\nYou can check the performance of the model on the training data\n\n\nlibrary(mixOmics)\ndata(srbct)\nX &lt;- srbct$gene\nY &lt;- srbct$class\nplsda_model&lt;-plsda(X,Y,ncomp = 2,scale = T)\nauroc(plsda_model, roc.comp = 2, print = FALSE)"
  },
  {
    "objectID": "slides_2.html#pls-da-needs-to-be-validated-1",
    "href": "slides_2.html#pls-da-needs-to-be-validated-1",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PLS-DA needs to be validated",
    "text": "PLS-DA needs to be validated\n\n\nBut PLS-DA is a supervised method….\n\nTo avoid overfitting, we need to validate it using cross-validation.\nCommon metrics include classification error rate, AUC, and Balanced Error Rate (BER).\nRepeated cross-validation gives a more stable estimate of performance.\nValidation helps decide the optimal number of components to retain.\n\n\n\nlibrary(mixOmics)\ndata(srbct)\nX &lt;- srbct$gene\nY &lt;- srbct$class\nplsda_model&lt;-plsda(X,Y,ncomp = 2,scale = T)\nset.seed(123)\nperf_plsda &lt;- perf(plsda_model, validation = \"Mfold\", \n                          folds = 5, nrepeat = 10, \n                          progressBar = FALSE, auc = TRUE)\nplot(perf_plsda, sd = TRUE,\n     legend.position = \"none\",dist = \"mahalanobis.dist\",measure = \"BER\")"
  },
  {
    "objectID": "slides_2.html#pls-da-7",
    "href": "slides_2.html#pls-da-7",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PLS-DA",
    "text": "PLS-DA\n\n\nAfter successful validation you can use PLS-DA to draw conclusion about the data.\n\nPlot the scores\nFind important variables\nFind relationship between the variables etc"
  },
  {
    "objectID": "slides_2.html#pls",
    "href": "slides_2.html#pls",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PLS",
    "text": "PLS\n\n\nPLS is the original algorithm\n\nIt is for regression\nY does not have to be a single vaiable. You can predict a matrix with PLS\nIt gives you the tools as PLS-DA\n\\(Q^2\\) can be used to assess the predictive ability of the model via cross-validation. It is like cross-validated \\(R^2\\)!\n\n\n\nlibrary(mixOmics)\ndata(liver.toxicity) # extract the liver toxicity data\nX &lt;- liver.toxicity$gene # use the gene expression data as the X matrix\nY &lt;- liver.toxicity$clinic # use the clinical data as the Y matrix\n\npls_model &lt;- pls(X = X, Y = Y, ncomp = 2, mode = 'regression')\nplotIndiv(pls_model)\n\n\n\n\n\n\n\nset.seed(123)\nperf_pls&lt;-perf(pls_model,folds = 5)\nperf_pls$measures$Q2.total$summary\n\n  feature comp       mean sd\n1       Y    1  0.2358257 NA\n2       Y    2 -0.1234260 NA"
  },
  {
    "objectID": "slides_2.html#data-integration",
    "href": "slides_2.html#data-integration",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Data integration",
    "text": "Data integration\n\n\nBoth PLS and PLS-DA can be extended for multi-omics integration.\n\nMultiple X blocks (e.g., transcriptomics, proteomics, metabolomics) can be integrated.\nDIABLO (Data Integration Analysis for Biomarker discovery using Latent variable approaches for Omics studies) is the framework used in mixOmics.\nIt identifies components that are shared across datasets and discriminate classes."
  },
  {
    "objectID": "slides_2.html#the-design-matrix-in-diablo",
    "href": "slides_2.html#the-design-matrix-in-diablo",
    "title": "Partial Least Squares (PLS) regression",
    "section": "The Design Matrix in DIABLO",
    "text": "The Design Matrix in DIABLO\n\n\nIn DIABLO, the design matrix controls how much each dataset (block) should be related to the others.\n\nIt is a square matrix, with one row and column per dataset (e.g., mRNA, miRNA, proteomics).\nThe diagonal is always 0 (a dataset doesn’t relate to itself).\nThe off-diagonal values (between 0 and 1) specify the strength of the connection between datasets:\n\n1 = force strong agreement between blocks\n0 = allow blocks to vary independently\nIntermediate values (e.g. 0.1) = weak to moderate connection\n\n\n\\[\nC =\n\\begin{bmatrix}\n0 & 1 & 0.5 \\\\\n1 & 0 & 0.2 \\\\\n0.5 & 0.2 & 0\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides_2.html#data-integration-1",
    "href": "slides_2.html#data-integration-1",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Data integration",
    "text": "Data integration\n\n\nDIABLO scores have the same meaning as PLS-DA but they have been optimized to show the shared structre between the datasets\n\ndata(breast.TCGA) # load in the data\n\n# set a list of all the X dataframes\ndata = list(miRNA = breast.TCGA$data.train$mirna, \n            mRNA = breast.TCGA$data.train$mrna,\n            proteomics = breast.TCGA$data.train$protein)\nY = breast.TCGA$data.train$subtype # set the response variable as the Y df\nsummary(Y)\n\nBasal  Her2  LumA \n   45    30    75 \n\ndesign = matrix(0.1, ncol = length(data), nrow = length(data), \n                dimnames = list(names(data), names(data)))\ndiag(design) = 0 # set diagonal to 0s\n\ndesign\n\n           miRNA mRNA proteomics\nmiRNA        0.0  0.1        0.1\nmRNA         0.1  0.0        0.1\nproteomics   0.1  0.1        0.0\n\ndiablo.model = block.plsda(X = data, Y = Y, ncomp = 2, design = design)\n\n\n\nplotIndiv(diablo.model, ind.names = FALSE, legend = TRUE, \n          title = 'DIABLO Sample Plots')"
  },
  {
    "objectID": "slides_2.html#data-integration-2",
    "href": "slides_2.html#data-integration-2",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Data integration",
    "text": "Data integration\n\n\nWe can check this shared stucture by finding correlation between the scores\n\n\nplotDiablo(diablo.model, ncomp = 1)"
  },
  {
    "objectID": "slides_2.html#data-integration-3",
    "href": "slides_2.html#data-integration-3",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Data integration",
    "text": "Data integration\n\n\nLoadings have the same interpretation as PLS-DA\n\n\nplotLoadings(diablo.model, comp = 2, contrib = 'max', method = 'median')"
  },
  {
    "objectID": "slides_2.html#data-integration-4",
    "href": "slides_2.html#data-integration-4",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Data integration",
    "text": "Data integration\n\n\nPLS-DA scores are on the same scale and in a shared space\n\nwe can sum them up or take the average\nor we can track the samaples\n\n\n\nplotArrow(diablo.model, ncomp = 1)"
  },
  {
    "objectID": "slides_2.html#data-integration-5",
    "href": "slides_2.html#data-integration-5",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Data integration",
    "text": "Data integration\n\n\nPLS-DA loadings are also on the same scale and in a shared space\n\nwe can use them to calculate relationships between different datasets\n\n\n\ncircosPlot(diablo.model, cutoff = 0.6, line = TRUE,\n           color.blocks= c('darkorchid', 'brown1', 'lightgreen'),\n           color.cor = c(\"chocolate3\",\"grey20\"), size.labels = 1.5)"
  },
  {
    "objectID": "slides_2.html#data-integration-6",
    "href": "slides_2.html#data-integration-6",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Data integration",
    "text": "Data integration\n\n\nMultiomics data integration also needs validation similar to PLS-DA\n\n\nset.seed(123)\nperf.diablo = perf(diablo.model, validation = 'Mfold', \n                   M = 10, nrepeat = 10, \n                   dist = 'mahalanobis.dist') \n\nplot(perf.diablo)"
  },
  {
    "objectID": "slides_2.html#data-integration-7",
    "href": "slides_2.html#data-integration-7",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Data integration",
    "text": "Data integration\n\n\nYou can even use this model to predict even if you don’t have all the datasets for the test sample!\n\n\ndata.test.TCGA = list(mRNA = breast.TCGA$data.test$mrna,\n                      miRNA = breast.TCGA$data.test$mirna)\n\npredict.diablo = predict(diablo.model, newdata = data.test.TCGA)\nconfusion.mat = get.confusion_matrix(truth = breast.TCGA$data.test$subtype,\n                     predicted = predict.diablo$WeightedVote$centroids.dist[,2])\nconfusion.mat\n\n      predicted.as.Basal predicted.as.Her2 predicted.as.LumA\nBasal                 18                 3                 0\nHer2                   0                14                 0\nLumA                   0                 0                35"
  },
  {
    "objectID": "slides_2.html#sparse-versions",
    "href": "slides_2.html#sparse-versions",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Sparse Versions",
    "text": "Sparse Versions\n\n\nSparse versions exist for PLS, PLS-DA, and DIABLO: they automatically select the most important variables (feature selection).\n\nyou can even tune the number of components and number of variables using tune.block.splsda\n\n\n\ntest.keepX = list (mRNA = 3, \n                   miRNA =3,\n                   proteomics = 3)\ndiablo.model = block.splsda(X = data, Y = Y, ncomp = 2, design = design,keepX = test.keepX)\nplotLoadings(diablo.model)"
  },
  {
    "objectID": "slides_2.html#diablo",
    "href": "slides_2.html#diablo",
    "title": "Partial Least Squares (PLS) regression",
    "section": "DIABLO",
    "text": "DIABLO\n\n\nDIABLO is a supervised multiblock method that:\n\nLearns components per dataset\nAligns them using a design matrix\nOptimizes both prediction and cross-dataset agreement\n\nThe design matrix is the core controller:\n\nHigh values = enforce agreement between datasets\nZero = no alignment required\n\nValidation = like PLS-DA, but across all blocks\nVariable plots (e.g., plotVar, network) help interpret shared vs unique signals"
  },
  {
    "objectID": "slides_2.html#final-thoughts-practical-tips",
    "href": "slides_2.html#final-thoughts-practical-tips",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Final Thoughts & Practical Tips",
    "text": "Final Thoughts & Practical Tips\n\n\n\nAlways center and scale your data\nChoose number of components via cross-validation or use mixOmics tuning function\nIn DIABLO, define a meaningful design matrix\nValidate using perf(), check stability and predictive performance\nBe cautious of overfitting, especially with small sample sizes\n\nset.seed(123)\nn_samples &lt;- 1000\nn_vars &lt;- 2000\n\nX_random &lt;- matrix(rnorm(n_samples * n_vars), nrow = n_samples, ncol = n_vars)\n\n# Assign random class labels (3 classes)\nY_random &lt;- factor(sample(c(\"A\", \"B\", \"C\"), size = n_samples, replace = TRUE))\n\n# Fit PLS-DA model\nplsda_random &lt;- plsda(X_random, Y_random, ncomp = 2)\n\n# Plot the sample projection\nplotIndiv(plsda_random, comp = 1:2, \n          group = Y_random, \n          legend = TRUE, \n          title = \"PLS-DA on Random Data\")"
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Agenda",
    "text": "Agenda\n\n\nMotivation: Why we need supervised dimensionality reduction\nPCA: Find directions of variance in X\nCCA: Maximize correlation between X and Y\nPLS: Maximize covariance between X and Y\nPLS-DA: Use PLS for classification tasks\nValidation: R², Q², cross-validation\nInterpretation: Loadings, scores, VIP scores\nData Integration: Multi-omics with block-wise PLS"
  },
  {
    "objectID": "index.html#why-learn-pls",
    "href": "index.html#why-learn-pls",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Why Learn PLS?",
    "text": "Why Learn PLS?\n\n\nHigh-dimensional data: \\(p \\gg n\\)\nStrong correlations among features\nNeed interpretable predictive components\nWant to relate X (predictors) to Y (response) where both can be matrices\nOmics data (genomics, transcriptomics, proteomics)\nChemometrics and quality control\nData integration across multiple blocks (multi-omics, multi-view)"
  },
  {
    "objectID": "index.html#lets-see-how-pca-works-under-the-hood",
    "href": "index.html#lets-see-how-pca-works-under-the-hood",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Let’s see how PCA works under the hood",
    "text": "Let’s see how PCA works under the hood\n\n\n\nPCA rotates the data so that the new axes align with the directions of maximum variance."
  },
  {
    "objectID": "index.html#lets-see-how-pca-works-under-the-hood-1",
    "href": "index.html#lets-see-how-pca-works-under-the-hood-1",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Let’s see how PCA works under the hood",
    "text": "Let’s see how PCA works under the hood\n\n\n\nPCA rotates the data so that the new axes align with the directions of maximum variance.\n\nThis is done using covariance matrix (ellipsoid)"
  },
  {
    "objectID": "index.html#lets-see-how-pca-works-under-the-hood-2",
    "href": "index.html#lets-see-how-pca-works-under-the-hood-2",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Let’s see how PCA works under the hood",
    "text": "Let’s see how PCA works under the hood\n\n\n\nPCA rotates the data so that the new axes align with the directions of maximum variance.\n\nThis is done using covariance matrix (ellipsoid)\n\nCalculate the major axis of the ellipsoid: compute the first eigenvector of the covariance matrix, it gives the direction of maximum variance."
  },
  {
    "objectID": "index.html#lets-see-how-pca-works-under-the-hood-3",
    "href": "index.html#lets-see-how-pca-works-under-the-hood-3",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Let’s see how PCA works under the hood",
    "text": "Let’s see how PCA works under the hood\n\n\n\nPCA rotates the data so that the new axes align with the directions of maximum variance.\n\nThis is done using covariance matrix (ellipsoid)\n\nCalculate the major axis of the ellipsoid: compute the first eigenvector of the covariance matrix, it gives the direction of maximum variance.\nReconstruct and deflate the data: project the data onto the first principal axis, then subtract this projection to remove the explained variance and reveal the remaining structure."
  },
  {
    "objectID": "index.html#lets-see-how-pca-works-under-the-hood-4",
    "href": "index.html#lets-see-how-pca-works-under-the-hood-4",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Let’s see how PCA works under the hood",
    "text": "Let’s see how PCA works under the hood\n\n\n\nPCA rotates the data so that the new axes align with the directions of maximum variance.\n\nThis is done using covariance matrix (ellipsoid)\n\nCalculate the major axis of the ellipsoid: compute the first eigenvector of the covariance matrix, it gives the direction of maximum variance.\nReconstruct and deflate the data: project the data onto the first principal axis, then subtract this projection to remove the explained variance and reveal the remaining structure.\nRepeat"
  },
  {
    "objectID": "index.html#more-technically",
    "href": "index.html#more-technically",
    "title": "Partial Least Squares (PLS) regression",
    "section": "More technically",
    "text": "More technically\n\n\nLet \\(X \\in \\mathbb{R}^{n \\times p}\\) be mean-centered data.\n\nCompute covariance matrix\n\\[\nS = \\frac{1}{n} X^{\\mathsf T} X\n\\]\nEigen decomposition: \\(S = V \\Lambda V^{\\mathsf T}\\)\n\nColumns of \\(V\\): principal directions (eigenvectors)\n\nDiagonal of \\(\\Lambda\\): variances (eigenvalues)\n\nExtract first component: \\(w_1 = \\text{first eigenvector}, \\quad t_1 = X w_1\\)\n\n\\(t_1\\) are the PC1 scores (projection)\n\nDeflate the data\n\\[\nX^{(1)} = X - t_1 w_1^{\\mathsf T}\n\\]\nRepeat on residuals\n\nPerform PCA again on \\(X^{(1)}\\)\n\nGet \\(w_2, t_2\\), deflate again\n\nContinue until desired number of components extracted\n\n\n\nAlternative: via Singular Value Decomposition (SVD)\nSVD of \\(X\\):\n\\[\nX = U D V^{\\mathsf T}\n\\]\n\nColumns of \\(V\\): loadings (same as eigenvectors of \\(S\\))\n\nColumns of \\(U D\\): scores (principal components)\n\nThis is equivalent and numerically more stable in high dimensions."
  },
  {
    "objectID": "index.html#variance-lives-on-the-diagonal",
    "href": "index.html#variance-lives-on-the-diagonal",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Variance Lives on the Diagonal",
    "text": "Variance Lives on the Diagonal\n\nThis is what PCA is really about it’s all sitting on the diagonal!\\[\nS = \\begin{bmatrix}\n\\color{red}{2.5} & 1.2 & 0.5 & 1.0 & 0.8 \\\\\n1.2 & \\color{red}{3.1} & 0.9 & 1.5 & 1.1 \\\\\n0.5 & 0.9 & \\color{red}{2.8} & 0.7 & 0.4 \\\\\n1.0 & 1.5 & 0.7 & \\color{red}{3.3} & 1.2 \\\\\n0.8 & 1.1 & 0.4 & 1.2 & \\color{red}{2.9}\n\\end{bmatrix}\n\\]\nDiagonal elements (in red):\n\\(\\text{Var}(X_1), \\text{Var}(X_2), \\dots, \\text{Var}(X_5)\\)\n\nOff-diagonal:\nHow features co-vary e.g., \\(\\text{Cov}(X_2, X_4) = 1.5\\)\nPCA finds a rotation of the axes such that:\n\nThe new covariance matrix is diagonal\n\nThe diagonal entries now represent the variance along the new axes\n\nPCA maximizes one diagonal value at a time\n\\[\n\\max_{w} \\quad w^{\\mathsf T} S w \\quad \\text{subject to } \\|w\\| = 1\n\\]"
  },
  {
    "objectID": "index.html#what-if-we-want-to-make-pca-supervised",
    "href": "index.html#what-if-we-want-to-make-pca-supervised",
    "title": "Partial Least Squares (PLS) regression",
    "section": "What if we want to make PCA supervised?",
    "text": "What if we want to make PCA supervised?\n\nLet’s say I have another dataset \\(Y\\), and I want PCA not just to show me the directions of highest variance in \\(X\\), but rather the directions in \\(X\\) that are most useful for explaining or predicting \\(Y\\).If we want to guide the eigen decomposition algorithm using \\(Y\\), we need to incorporate information about \\(Y\\) into the square matrix (which we previously called the covariance matrix) that drives the decomposition.In PCA, this matrix is \\(X^\\top X\\), which only contains information about variance within \\(X\\). To bring \\(Y\\) into the picture, we need a new matrix that reflects how \\(X\\) and \\(Y\\) interact. One natural choice is the cross-covariance matrix \\(X^\\top Y\\), which tells us how each variable in \\(X\\) co-varies with each variable in \\(Y\\)."
  },
  {
    "objectID": "index.html#the-cross-covariance-matrix",
    "href": "index.html#the-cross-covariance-matrix",
    "title": "Partial Least Squares (PLS) regression",
    "section": "The Cross-Covariance Matrix",
    "text": "The Cross-Covariance Matrix\n\n\nSuppose we have two centered datasets:\n\n\\(X \\in \\mathbb{R}^{n \\times p}\\): predictors\n\n\\(Y \\in \\mathbb{R}^{n \\times q}\\): responses\n\nWe define the cross-covariance matrix between \\(X\\) and \\(Y\\) as:\n\\[\nC_{XY} = \\frac{1}{n} X^\\top Y\n\\]\nThis matrix has shape \\(p \\times q\\), and each entry\\((i, j)\\) shows how much feature \\(X_i\\) co-varies with feature \\(Y_j\\) across the samples.\n\n\n\n         [,1]        [,2]\n[1,] 3.150913 -1.06559666\n[2,] 2.199966 -0.07108427\n[3,] 1.145256 -0.39498518\n\n\n\n\n\n\n\n\n\n\nThe cross-covariance matrix captures the linear relationships between variables in \\(X\\) and variables in \\(Y\\)\nIt contains exactly the information we want when we’re trying to find directions in \\(X\\) that are most predictive of \\(Y\\)"
  },
  {
    "objectID": "index.html#fixing-the-square-matrix",
    "href": "index.html#fixing-the-square-matrix",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Fixing the Square Matrix",
    "text": "Fixing the Square Matrix\n\nThe cross-covariance matrix \\(X^\\top Y\\) has shape \\(p \\times q\\), so we can’t diagonalize it directly. But we can build a square matrix from it that still captures the relationship between \\(X\\) and \\(Y\\).Let’s define:\\[\nM = X^\\top Y Y^\\top X\n\\]\nThis matrix has shape \\(p \\times p\\), just like the covariance matrix \\(X^\\top X\\) in PCA.\nIt is symmetric and positive semi-definite, so we can perform eigen decomposition.\nThink of this matrix as measuring: “How strongly does each direction in \\(X\\) contribute to predicting \\(Y\\), and how redundant are those directions with each other?”"
  },
  {
    "objectID": "index.html#whats-on-the-diagonal-and-off-diagonal-in-m",
    "href": "index.html#whats-on-the-diagonal-and-off-diagonal-in-m",
    "title": "Partial Least Squares (PLS) regression",
    "section": "What’s on the Diagonal and Off-Diagonal in \\(M\\)",
    "text": "What’s on the Diagonal and Off-Diagonal in \\(M\\)\n\n\nLet’s interpret the entries of \\(M = X^\\top Y Y^\\top X\\):\n\nDiagonal entries \\(M_{ii}\\) measure how strongly feature \\(X_i\\) is associated with all of Y, aggregated across all response variables and samples.\n\nIf \\(M_{11}\\) is large, it means \\(X_1\\) is very useful in predicting \\(Y\\).\nThese are signal strengths for individual features in \\(X\\).\n\nOff-diagonal entries \\(M_{ij}\\) measure shared predictive power between \\(X_i\\) and \\(X_j\\).\n\nIf \\(M_{12}\\) is large, it means \\(X_1\\) and \\(X_2\\) are both predictive of similar aspects of \\(Y\\).\nThese reflect redundancy or correlation in how features contribute to predicting \\(Y\\).\n\n\nSo in a sense:\n\nDiagonal = importance of each feature for predicting \\(Y\\)\nOff-diagonal = overlap in what features predict about \\(Y\\)"
  },
  {
    "objectID": "index.html#example",
    "href": "index.html#example",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "index.html#example-cross-covariance",
    "href": "index.html#example-cross-covariance",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Example (cross covariance)",
    "text": "Example (cross covariance)\nNow we calculate the cross covariance and plot it:"
  },
  {
    "objectID": "index.html#example-eigenvectors",
    "href": "index.html#example-eigenvectors",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Example (eigenvectors)",
    "text": "Example (eigenvectors)\n\n\n\\(X^\\top Y Y^\\top X\\) gives components in the X-space.\nTo get components in the Y-space, use \\(Y^\\top X X^\\top Y\\).\nBoth capture the same cross-covariance but in their own coordinate systems.\nRotate both \\(X\\) and \\(Y\\) to find aligned directions in each space."
  },
  {
    "objectID": "index.html#final-deflation",
    "href": "index.html#final-deflation",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Final deflation",
    "text": "Final deflation\n\n\\[\nX_{\\text{new}} = X - \\mathbf{t} \\mathbf{p}^\\top,\n\\quad \\text{where } \\mathbf{p} = \\frac{X^\\top \\mathbf{t}}{\\mathbf{t}^\\top \\mathbf{t}}\n\\]\\[\nY_{\\text{new}} = Y - \\mathbf{t} \\mathbf{q}^\\top,\n\\quad \\text{where } \\mathbf{q} = \\frac{Y^\\top \\mathbf{t}}{\\mathbf{t}^\\top \\mathbf{t}}\n\\]\nX is deflated using its own projection \\(\\mathbf{t}\\) → removes variation already captured (as in PCA).\nY is deflated using the same \\(\\mathbf{t}\\) from X → ensures we only keep what is not yet explained by X.\nAfter we do the deflation we redo the covariance calculation and start from the beginning."
  },
  {
    "objectID": "index.html#final-deflation-1",
    "href": "index.html#final-deflation-1",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Final deflation",
    "text": "Final deflation\n\n\n\\(X^\\top Y Y^\\top X\\) gives components in the X-space.\nTo get components in the Y-space, use \\(Y^\\top X X^\\top Y\\).\nBoth capture the same cross-covariance but in their own coordinate systems.\nRotate both \\(X\\) and \\(Y\\) to find aligned directions in each space."
  },
  {
    "objectID": "index.html#projecting-x-and-y-into-latent-space-pls",
    "href": "index.html#projecting-x-and-y-into-latent-space-pls",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Projecting X and Y into Latent Space (PLS)",
    "text": "Projecting X and Y into Latent Space (PLS)\n\nGet the weights\nX weight (from eigen decomposition):\n\\[\n\\mathbf{w} = \\text{first eigenvector of } X^\\top Y Y^\\top X\n\\]\nY weight (aligned with \\(\\mathbf{t}\\)):\n\\[\n\\mathbf{q} = \\frac{Y^\\top \\mathbf{t}}{\\mathbf{t}^\\top \\mathbf{t}},\n\\quad \\text{where } \\mathbf{t} = X \\mathbf{w}\n\\]\nProject X and Y\nLatent score from X:\n\\[\n\\mathbf{t} = X \\mathbf{w}\n\\]\nLatent score from Y:\n\\[\n\\mathbf{u} = Y \\mathbf{q}\n\\] \\(\\mathbf{t}\\) captures direction in \\(X\\) most predictive of \\(Y\\). \\(\\mathbf{u}\\) is the matching direction in \\(Y\\), maximally aligned with \\(\\mathbf{t}\\)."
  },
  {
    "objectID": "index.html#projection",
    "href": "index.html#projection",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Projection",
    "text": "Projection\n\nIn R we can use mixOmics package to sort out everything for us\nlibrary(mixOmics)\npls_model&lt;-pls(X = X,Y = Y,ncomp = 2)\nplotIndiv(pls_model)"
  },
  {
    "objectID": "index.html#partial-least-squares-pls",
    "href": "index.html#partial-least-squares-pls",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Partial Least Squares (PLS)",
    "text": "Partial Least Squares (PLS)\n\n\nMaximize the covariance between \\(X\\mathbf{w}\\) and \\(Y\\mathbf{q}\\)\nLearn latent components that are predictive and low-dimensional\nLet:\n\\(\\mathbf{w}\\): weight vector in \\(X\\)-space\n\\(\\mathbf{q}\\): weight vector in \\(Y\\)-space\n\\(\\mathbf{t} = X \\mathbf{w}\\): latent score from \\(X\\)\n\\(\\mathbf{u} = Y \\mathbf{q}\\): latent score from \\(Y\\)\n\\[\n\\max_{\\|\\mathbf{w}\\| = 1, \\|\\mathbf{q}\\| = 1} \\; \\text{Cov}(X \\mathbf{w}, Y \\mathbf{q}) = \\mathbf{w}^\\top X^\\top Y \\mathbf{q}\n\\]Latent components:\\[\n  \\mathbf{T} = X \\mathbf{W}, \\quad \\mathbf{U} = Y \\mathbf{Q}\n  \\] * Can be used for regression:\\[\n  Y \\approx \\mathbf{T} \\mathbf{B}, \\quad \\text{where } \\mathbf{B} = (\\mathbf{T}^\\top \\mathbf{T})^{-1} \\mathbf{T}^\\top Y\n  \\]"
  },
  {
    "objectID": "index.html#validation-of-pls",
    "href": "index.html#validation-of-pls",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Validation of PLS",
    "text": "Validation of PLS\n\n\nLike any other supervised method, we must validate PLS before interpreting or using it for prediction.\nWe typically assess:\nR²X — Explained Variance in X\n\\[\nR^2_X = \\frac{\\text{Variance explained by the components in } X}{\\text{Total variance in } X}\n\\]\n\nShows how well the model summarizes the predictors.\n\nR²Y:Explained Variance in Y\n\\[\nR^2_Y = \\frac{\\text{Variance explained by the components in } Y}{\\text{Total variance in } Y}\n\\]\n\nReflects how well \\(Y\\) is fitted by the model.\nHigher \\(R^2_Y\\) means better in-sample fit.\n\n\nQ²: Predictive Ability (Cross-Validated \\(R^2_Y\\))\n\\[\nQ^2 = 1 - \\frac{\\sum_{k=1}^{Q} \\text{PRESS}^h_k}{\\sum_{k=1}^{Q} \\text{RSS}^0_k}\n\\]\nWhere:\n\n\\(\\text{PRESS}^h_k = \\sum_{i=1}^{n} (y_i^{(h)} - \\hat{y}_i^{(h)(-i)})^2\\): Prediction error for each Y-variable \\(k\\) using leave-one-out or K-fold CV.\n\\(\\text{RSS}^0_k = \\sum_{i=1}^{n} (y_i^{(h)} - \\bar{y}_k)^2\\): Residual sum of squares using the mean of Y as predictor (null model).\n\nThis \\(Q^2\\) assesses the marginal contribution of component \\(h\\) to the model’s predictive power, as defined by Tenenhaus (1998) and implemented by Chavent & Patouille (2003).\nA rule of thumb:\n\\[\nQ^2 \\geq 1 - 0.95^2 = 0.0975\n\\]\nmeans the component contributes meaningfully to prediction."
  },
  {
    "objectID": "index.html#validation-of-pls-1",
    "href": "index.html#validation-of-pls-1",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Validation of PLS",
    "text": "Validation of PLS\n\n\npls_model&lt;-pls(X = X,Y = Y,ncomp =3,mode = \"regression\",scale = FALSE)\nperf_pls&lt;-perf(pls_model,validation = \"loo\")\nprint(1-cumprod(1-perf_pls$measures$Q2.total$values$value))\n\n[1] 0.4237092 0.7695074 0.7875983\n\nplot(perf_pls,\"Q2.total\")"
  },
  {
    "objectID": "index.html#variable-importance-vip",
    "href": "index.html#variable-importance-vip",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Variable importance (VIP)",
    "text": "Variable importance (VIP)\n\n\nVIP scores tell us how important each predictor (X-variable) is in explaining or predicting the outcome Y.\nIt combines:\n\nHow much each component explains Y (via \\(R^2_Y\\))\nHow strongly each variable contributes to the components (via squared loadings \\(w_{jh}^2\\))\n\n\\[\n\\text{VIP}_j = \\sqrt{p \\cdot \\sum_{h=1}^{H} \\left( \\frac{R^2_{Y,h}}{\\sum_{h=1}^{H} R^2_{Y,h}} \\cdot w_{jh}^2 \\right)}\n\\]Where:\n\\(p\\): number of variables in X\n\\(H\\): number of components\n\\(R^2_{Y,h}\\): how well component \\(h\\) explains Y\n\\(w_{jh}\\): loading (weight) of variable \\(j\\) in component \\(h\\)\n\n✔ VIP &gt; 1 → variable is important ✘ VIP &lt; 1 → less contribution to prediction"
  },
  {
    "objectID": "index.html#example-in-r",
    "href": "index.html#example-in-r",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Example in R",
    "text": "Example in R\n\n\nvip_pls_model&lt;-vip(pls_model)\nprint(vip_pls_model)\n\n       comp1     comp2     comp3\nX1 1.1670428 1.2144690 1.1956095\nX2 0.8706446 0.9035354 0.9128069\nX3 0.9380773 0.8418366 0.8586626"
  },
  {
    "objectID": "index.html#pls",
    "href": "index.html#pls",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PLS",
    "text": "PLS\n\n\nPLS is a supervised projection method: Finds directions in \\(X\\) that best predict \\(Y\\).\nIt handles:\n\nCollinearity in predictors\nMultivariate responses\nHigh-dimensional data (\\(p \\gg n\\))\n\nOutputs:\n\nLatent components \\(\\mathbf{T} = XW\\)\nRegression model \\(Y \\approx \\mathbf{T}B\\)\n\nCan be validated using: \\(R^2_X\\), \\(R^2_Y\\), and \\(Q^2\\)"
  },
  {
    "objectID": "index.html#partial-least-squares-discriminant-analysis-pls-da",
    "href": "index.html#partial-least-squares-discriminant-analysis-pls-da",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Partial Least Squares Discriminant Analysis (PLS-DA)",
    "text": "Partial Least Squares Discriminant Analysis (PLS-DA)\n\n\nPLS-DA is just PLS applied to a categorical Y\n\nWe encode class labels as dummy variables (e.g., one-hot)\n\nThe rest is the same:\n\nUse PLS to find components that best separate classes\nProjection is guided by supervised information\n\nInstead of \\(R^2_Y\\), we evaluate using:\n\nAccuracy, AUC, confusion matrix, cross-validation error\n\n\n\n\nSample\nClass Label\nOne-Hot Encoded \\(Y\\)\n\n\n\n\n1\nA\n(1, 0, 0)\n\n\n2\nB\n(0, 1, 0)\n\n\n3\nC\n(0, 0, 1)\n\n\n4\nA\n(1, 0, 0)"
  },
  {
    "objectID": "index.html#example-in-r-1",
    "href": "index.html#example-in-r-1",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Example in R",
    "text": "Example in R\n\nI have simulated some data based on 3 classes and the idea is to see if we can “separate” the classes using PLSDA\nplsda_model&lt;-plsda(X,class_labels)\nplotIndiv(plsda_model)"
  },
  {
    "objectID": "index.html#model-validation",
    "href": "index.html#model-validation",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Model validation",
    "text": "Model validation\n\nDo NOT trust PLSDA scores. Always validate your model using cross validation\nplsda_model_perf&lt;-perf(plsda_model)\nplot(plsda_model_perf)"
  },
  {
    "objectID": "index.html#x-y-space-and-prediction",
    "href": "index.html#x-y-space-and-prediction",
    "title": "Partial Least Squares (PLS) regression",
    "section": "X, Y space and prediction",
    "text": "X, Y space and prediction\n\nWe use a distance matrix in \\(Y\\) to do classification! When a new sample comes. we first project to \\(X\\) then from \\(X\\)-space to \\(Y\\) using regression (PLS) and measure the distances to the center of the classes. The closest one will be the predicted classes.\npar(mfrow=c(1,2))\nplot(plsda_model$variates$X,col=as.factor(class_labels),main=\"X\")\nplot(plsda_model$variates$Y,col=as.factor(class_labels),main=\"Y\")"
  },
  {
    "objectID": "index.html#variable-selection",
    "href": "index.html#variable-selection",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Variable selection",
    "text": "Variable selection\n\n\nIn mixOmics, the correlation plot shows how strongly each variable in \\(X\\) is correlated with the PLS-DA components.\nVariables with high absolute correlation are likely important for discriminating between classes.\nYou can use this to visually select variables that contribute most to class separation.\nWorks well with sparse PLS-DA (splsda) to highlight variables with strongest group-specific signals.\n\nplotVar(plsda_model)"
  },
  {
    "objectID": "index.html#multiomics-data-diablo",
    "href": "index.html#multiomics-data-diablo",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Multiomics data (DIABLO)",
    "text": "Multiomics data (DIABLO)\n\n\nA multiblock extension of PLS for integrating multiple datasets (e.g., transcriptomics, proteomics, metabolomics).\nLearns components for each dataset that:\n\nPredict the outcome (Y), and\nAre maximally correlated across datasets, based on prior expectations."
  },
  {
    "objectID": "index.html#pls-for-multiomics-data",
    "href": "index.html#pls-for-multiomics-data",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PLS for multiomics data",
    "text": "PLS for multiomics data\n\nLet’s say we have 3 datasets \\(A\\), \\(B\\) and \\(C\\). and we have \\(Y\\) (our outcome) We do PLS on each dataset separate and extract only the first components for each dataset:\\[\nt_A^{(1)},\\ t_B^{(1)},\\ t_C^{(1)}\\ \\text{(from PLS)}\n\\]Let’s say we want to guide each dataset’s component using how much we expect it to co-vary with the others.\nThis is called in the design matrix \\(C\\):\\[\nC =\n\\begin{bmatrix}\n0 & 1 & 0.5 \\\\\n1 & 0 & 0.2 \\\\\n0.5 & 0.2 & 0\n\\end{bmatrix}\n\\]Weighted combination (for sample \\(i\\)) give us an average score:\\[\nz_i^{(q)} = \\sum_{k \\neq q} C_{qk} \\cdot t_i^{(k)}\n\\]"
  },
  {
    "objectID": "index.html#pls-for-multiomics-data-1",
    "href": "index.html#pls-for-multiomics-data-1",
    "title": "Partial Least Squares (PLS) regression",
    "section": "PLS for multiomics data",
    "text": "PLS for multiomics data\n\nTo update loadings for dataset \\(q\\), we compute:\\[\nw_q^{\\text{new}} = \\frac{X_q^\\top z^{(q)}}{\\| X_q^\\top z^{(q)} \\|}\n\\]We compute the new latent component:\\[\nt_q^{\\text{new}} = X_q w_q^{\\text{new}}\n\\]This is the projected value of each sample in dataset \\(q\\) along the new direction. We use these set of scores and weights for deflating the original datasets. The idea is that we want to explain variance in \\(Y\\) using the shared information across the datasets!"
  },
  {
    "objectID": "index.html#diablo-in-r",
    "href": "index.html#diablo-in-r",
    "title": "Partial Least Squares (PLS) regression",
    "section": "DIABLO in R",
    "text": "DIABLO in R\n\nI have simulate two more datasets now we have \\(X\\), \\(X2\\) and \\(X3\\):\n# Create named list for DIABLO\ndata_list &lt;- list(X1 = X, X2 = X2, X3 = X3)\n\n# create design matrix:\n\ndesign &lt;- matrix(c(0, 0.8, 0.5,\n                   0.8, 0, 0.3,\n                   0.5, 0.3, 0), nrow = 3)\n\nprint(design)\n\n     [,1] [,2] [,3]\n[1,]  0.0  0.8  0.5\n[2,]  0.8  0.0  0.3\n[3,]  0.5  0.3  0.0"
  },
  {
    "objectID": "index.html#diablo-in-r-1",
    "href": "index.html#diablo-in-r-1",
    "title": "Partial Least Squares (PLS) regression",
    "section": "DIABLO in R",
    "text": "DIABLO in R\n\nWe can check whether the correlation between components from each data set has been maximised as specified in the design matrix.\ndiablo_model &lt;- block.plsda(X = data_list, Y = class_labels, design = design, ncomp = 2)\nplotDiablo(diablo_model, ncomp = 1)"
  },
  {
    "objectID": "index.html#diablo-in-r-2",
    "href": "index.html#diablo-in-r-2",
    "title": "Partial Least Squares (PLS) regression",
    "section": "DIABLO in R",
    "text": "DIABLO in R\n\nWe can project each sample into the space spanned by the components of each block. Clustering of the samples can be better assessed with this plot.\ndiablo_model &lt;- block.plsda(X = data_list, Y = class_labels, design = design, ncomp = 2)\nplotIndiv(diablo_model)"
  },
  {
    "objectID": "index.html#validation",
    "href": "index.html#validation",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Validation",
    "text": "Validation\n\nSimilar to PLS-DA, DIABLO model must be validated. The predictions however are now average prediction or majority of votes between different datasets.\ndiablo_model_perf &lt;- perf(diablo_model)\n\nplot(diablo_model_perf)"
  },
  {
    "objectID": "index.html#similarities-across-modalities",
    "href": "index.html#similarities-across-modalities",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Similarities across modalities",
    "text": "Similarities across modalities\n\nThe best starting point to evaluate the correlation structure between variables is with the correlation circle plot.\nplotVar(diablo_model, var.names = FALSE, \n        style = 'graphics', legend = TRUE,\n        pch = c(16, 17, 15), cex = c(2,2,2), \n        col = c('darkorchid', 'brown1', 'lightgreen'))"
  },
  {
    "objectID": "index.html#diablo",
    "href": "index.html#diablo",
    "title": "Partial Least Squares (PLS) regression",
    "section": "DIABLO",
    "text": "DIABLO\n\n\nDIABLO is a supervised multiblock method that:\n\nLearns components per dataset\nAligns them using a design matrix\nOptimizes both prediction and cross-dataset agreement\n\nThe design matrix is the core controller:\n\nHigh values = enforce agreement between datasets\nZero = no alignment required\n\nValidation = like PLS-DA, but across all blocks\nVariable plots (e.g., plotVar, network) help interpret shared vs unique signals"
  },
  {
    "objectID": "index.html#final-thoughts-practical-tips",
    "href": "index.html#final-thoughts-practical-tips",
    "title": "Partial Least Squares (PLS) regression",
    "section": "Final Thoughts & Practical Tips",
    "text": "Final Thoughts & Practical Tips\n\n\n\nAlways center and scale your data\nChoose number of components via cross-validation or use mixOmics tuning function\nIn DIABLO, define a meaningful design matrix\nValidate using perf(), check stability and predictive performance\nBe cautious of overfitting, especially with small sample sizes\n\nset.seed(123)\nn_samples &lt;- 1000\nn_vars &lt;- 2000\n\nX_random &lt;- matrix(rnorm(n_samples * n_vars), nrow = n_samples, ncol = n_vars)\n\n# Assign random class labels (3 classes)\nY_random &lt;- factor(sample(c(\"A\", \"B\", \"C\"), size = n_samples, replace = TRUE))\n\n# Fit PLS-DA model\nplsda_random &lt;- plsda(X_random, Y_random, ncomp = 2)\n\n# Plot the sample projection\nplotIndiv(plsda_random, comp = 1:2, \n          group = Y_random, \n          legend = TRUE, \n          title = \"PLS-DA on Random Data\")"
  }
]